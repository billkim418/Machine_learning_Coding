## Chapter 2 : Kernel-based Learning

ì´ë²ˆ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ ì˜ ì›ë¦¬ë¥¼ ë‹¤ë£¨ê³  ë” ë‚˜ì•„ê°€ ë¹„ìŠ·í•œ ì„ í˜• ë¶„ë¥˜ê¸°ì¸ LDAì™€ì˜ ë¹„êµë¥¼ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤. ìš°ì„ ì ìœ¼ë¡œ í•´ë‹¹ ê¸€ì€ ê¸°ë³¸ì ìœ¼ë¡œ 
[ê³ ë ¤ëŒ€í•™êµ ê°•í•„ì„± êµìˆ˜ë‹˜](https://github.com/pilsung-kang)ì˜ ìˆ˜ì—…ì„ ë“£ê³  ì‘ì„±í–ˆìŒì„ ë°í™ë‹ˆë‹¤.

---
### Chapter
- í•´ë‹¹ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì€ ìˆœì„œëŒ€ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.
1. SVM ì´ë¡ ì  ì›ë¦¬ 
2. SVMì—ì„œì˜ Kernel ì‘ìš©
3. Kernel Fisher Dsicriminant Anlysis

### SVMì˜ ì´ë¡ ì  ì›ë¦¬ ë° êµ¬í˜„
- ìš°ì„ ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” SVMì´ ì„ í˜• ë¶„ë¥˜ê¸°ë¼ëŠ” ê²ƒì„ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì€ ê·¸ë¦¼ì´ ì£¼ì–´ì ¸ ìˆì„ ë•Œ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ë¶„ë¥˜í•´ì•¼ í• ê¹Œìš”? 
![02_1_Kernel-based Learning_SVM](https://user-images.githubusercontent.com/68594529/199439258-9627e91a-51bf-4a27-a532-c37586c78e40.png)

Q : Bì˜ ê·¸ë¦¼ì²˜ëŸ¼ ë‚˜ëˆ„ëŠ”ê²ƒì´ ì •ë§ ì¢‹ì€ ë°©ë²•ì´ë¼ëŠ” ê·¼ê±°ê°€ ìˆì„ê¹Œìš”?<br>
A : ì´ëŸ¬í•œ ê·¼ê±°ë¥¼ ë§ˆë ¨í•˜ê¸° ìœ„í•´ì„œ VC_dimensionì´ë¼ëŠ” ê°œë…ì„ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤. VC dimensionì´ë€ íŠ¹ì • í•¨ìˆ˜ê°€ ì–¼ë§ˆë‚˜ ë³µì¡í•œì§€ ì¦‰ Capacityë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì¦‰ í•¨ìˆ˜ Hì— ì˜í•´ ìµœëŒ€ë¡œ shatter ê°€ëŠ¥í•œ ì ì˜ ìˆ«ìê°€ ê³§ VC dimensionì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- ì—¬ê¸°ì„œ shatterì˜ ì˜ë¯¸ë¥¼ ì„¤ëª…í•˜ê¸° ì „ì— Dichotomyë€ ê°œë…ì„ ì¶”ê°€ì ìœ¼ë¡œ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.ê°„ë‹¨íˆ Dichotomyë€ íŠ¹ì • ì§‘í•©ì´ ìˆë‹¤ë©´ ì´ë¥¼ ì´ë¶„ë²•ì ìœ¼ë¡œ ë‚˜ëˆˆë‹¤ëŠ” ê°œë…ì…ë‹ˆë‹¤. ì˜ˆë¥¼ë“¤ì–´ A, B ,C ë¼ëŠ” ì ì´ ì¡´ì¬í•œë‹¤ë©´ ì´ëŠ” ì´ {A}, {B}, {C}, {A,B}, {A,C}, {B,C}, {A,B,C} ì´ 8ê°€ì§€ì˜ êµ¬ë¶„ë˜ëŠ” ê²½ìš°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ì²˜ëŸ¼ íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” setìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ Dichotomyë¼ê³  í‘œí˜„í•©ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ shatterí•˜ë‹¤ëŠ” ê²ƒì€ ë¬´ìŠ¨ ì˜ë¯¸ì¼ê¹Œìš”? ë°”ë¡œ íŠ¹ì • í•¨ìˆ˜ê°€ Dichotomyë¥¼ ëª¨ë‘ í‘œí˜„í• ìˆ˜ ìˆëŠëƒì…ë‹ˆë‹¤. ê°€ëŠ¥í•˜ë‹¤ë©´ shatterí•˜ë‹¤ê³  í‘œí˜„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

- ì—¬ê¸°ê¹Œì§€ VC dimensionì˜ ê°œë…ì— ëŒ€í•´ ì•Œì•„ë´¤ëŠ”ë°ìš”. ê·¸ë ‡ë‹¤ë©´ SVMì€ ì–´ë–¤ ì‹ìœ¼ë¡œ ì„ ì„ ë¶„ë¦¬í•˜ëŠ” ê²ƒì¼ê¹Œìš”? ì •ë‹µì€ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ marginì„ ìµœëŒ€í™” í•˜ëŠ” ì‹ìœ¼ë¡œ ë¶„ë¦¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/68594529/199475366-bb87c54f-0f2c-4336-85d7-66217327d190.png)

- maringì´ë€ í•´ë‹¹ ë¶„ë¥˜ê²½ê³„ë©´ìœ¼ë¡œë¶€í„° ê°€ì¥ ê°€ê¹Œìš´ ì ë“¤ê³¼ì˜ ê±°ë¦¬ë¡œ ì •ì˜ë˜ê³  ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì€ ì´ë¥¼ ë²•ì„ ë²¡í„°ë¥¼ ì´ìš©í•´ í‘œí˜„í•œ ê·¸ë¦¼ì…ë‹ˆë‹¤.

- Q : ì—¬ê¸°ì„œ í•œê°€ì§€ ì˜ë¬¸ì ì´ ìƒê¸°ëŠ”ë° ê³¼ì—° ë§ˆì§„ì´ ìµœëŒ€í™”ê°€ ë˜ë©´ VC dimensionì´ ìµœì†Œí™”ê°€ ë ê¹Œìš”?
- A : êµ¬ì¡°ì  ìœ„í—˜ ìµœì†Œí™”(Structural Risk Minimization) ì ‘ê·¼ë²•ì„ í†µí•´ í•´ê²° ê°€ëŠ¥í•©ë‹ˆë‹¤.

 - êµ¬ì¡°ì  ìœ„í—˜ ìµœì†Œí™”ì˜ ìˆ˜ì‹ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/68594529/199477397-71e4d44e-e204-4e98-8562-80af5b700869.png)

- VC dimensionì˜ ìˆ˜ì‹ì˜ uppber boundëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/68594529/199479054-09142359-a748-44d8-ac12-bc6aaa069deb.png)

í•´ë‹¹ 2ê°€ì§€ ìˆ˜ì‹ì„ ì¡°í•©í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ ê²°ë¡ ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ì„ ì ìœ¼ë¡œ 
$âˆ†^2 â†‘ -> âŒˆ\frac{R^2}{âˆ†^2} âŒ‰ â†“ -> min( âŒˆ\frac{R^2}{âˆ†^2} âŒ‰,D) â†“ -> h â†“ ->  B â†“ -> R[f] â†“ $

ë”°ë¼ì„œ ë§ˆì§„ì´ ìµœëŒ€í™” ë˜ë©´ VC dimensionì´ ìµœì†Œí™”ë˜ê³  ì´ëŠ” ì¦‰ Capacity í•­ì´ ìµœì†Œí™”ë˜ê²Œ ë©ë‹ˆë‹¤.

ì—¬ê¸°ê¹Œì§€ ì´í•´í•˜ì…¨ë‹¤ë©´ ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ SVMì„ ëª¨ë¸ë§í•´ ë³´ê² ìŠµë‹ˆë‹¤.<br>
SVM Modeling process
1. ë§ˆì§„ì´ ìµœëŒ€í™” ë˜ëŠ” ëª©ì  í•¨ìˆ˜ ì„¤ì • $min 1/2 â€–ğ‘¤â€–^2 +\sum_{i=1}^ n Î¾_ğ‘–$
2. ëª¨ë“  ë°ì´í„°ì— ëŒ€í•˜ì—¬ ì œì•½ì‹ ì„¤ì •s.t  $ğ‘¦_ğ‘– (ğ‘¤^ğ‘‡ ğ‘¥_ğ‘–+ğ‘)â‰¥1âˆ’Î¾_ğ‘–,  Î¾_ğ‘–â‰¥0,  âˆ€ğ‘–$
3. ìµœì í™”ë¥¼ í†µí•œ ë¬¸ì œ í•´ê²° : ë¼ê·¸ë‘ì£¼, KKT conditions ë“±ì„ ì´ìš©í•œë‹¤ -> ìì„¸í•œ ë‚´ìš©ì€ ìµœì í™” ë‚´ìš©ì´ë¯€ë¡œ ìƒëµí•˜ê³  ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.

í•´ë‹¹ ì „ê°œ ê³¼ì •ì„ ë³´ë©´ ê°‘ìê¸° Î¾_ğ‘–ê°€ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. í•´ë‹¹ ê¸°í˜¸ëŠ” í¬ì‚¬ì´(Penalty)ë¡œì„œ ëª¨ë“  ë°ì´í„°ì—ëŠ” ë…¸ì´ì¦ˆ(noise)ê°€ ì¡´ì¬í•˜ê³  ì´ë¥¼ ë°˜ì˜í•˜ê¸° ìœ„í•œ ì¥ì¹˜ì…ë‹ˆë‹¤!

<img src="https://user-images.githubusercontent.com/68594529/199502831-9c1e0b1a-2b0f-4561-a366-a14aaeba1207.png" width="600" height="400"/>

ìœ„ì˜ ê·¸ë¦¼ì²˜ëŸ¼ ìš°ë¦¬ëŠ” íŒ¨ë„í‹°ë¥¼ ì¤Œìœ¼ë¡œì¨ ì¢€ë” softí•˜ê²Œ SVMì„ ì„¤ê³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì²˜ëŸ¼ SVMì—ëŠ” í¬ì‚¬ì´ ì´ì™¸ì—ë„ Performanceë¥¼ ìœ„í•œ ì¥ì¹˜ë“¤ì´ ë” ìˆìŠµë‹ˆë‹¤.<br>
ì˜ˆë¥¼ ë“¤ë©´, ì˜¤ë¶„ë¥˜ ë¹„ìš© C ì¦‰ íŒ¨ë„í‹°ë¥¼ ì¤„ì´ë©´ì„œ ë§ˆì§„ê¹Œì§€ ê°™ì´ ì¤„ì¼ ê²ƒì¸ì§€ í˜¹ì€ íŒ¨ë„í‹°ë¥¼ ë°›ë”ë¼ë„ ë§ˆì§„ì„ ë„“ê²Œ ì¡ë„ë¡ í•™ìŠµì‹œí‚¬ê²ƒì¸ì§€ ì¡°ì ˆ ê°€ëŠ¥í•©ë‹ˆë‹¤

### SVMì—ì„œì˜ Kernel ì‘ìš©

Q : ì²˜ìŒì— SVMì´ ì„ í˜• ë¶„ë¥˜ê¸°ë¼ëŠ” ì–˜ê¸°ë¥¼ í•˜ì…¨ëŠ”ë° ê·¸ë ‡ë‹¤ë©´ ë¹„ì„ í˜•ì ì¸ ë°ì´í„°ëŠ” ì–´ë–»ê²Œ í•´ê²°í• ìˆ˜ ìˆì„ê¹Œìš”?<br>
A : ì›ë˜ ê³µê°„ì´ ì•„ë‹Œ ì„ í˜• ë¶„ë¥˜ê°€ ê°€ëŠ¥í•œ ë” ê³ ì°¨ì›ì˜ ê³µê°„ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë³´ë‚´ì„œ(mappin) í›„ì— ì´ë¥¼ í•™ìŠµí•˜ë©´ ë©ë‹ˆë‹¤. í•´ë‹¹ ê³¼ì •ì—ì„œ ì»¤ë„í•¨ìˆ˜ê°€ ì‘ìš©ì„ í•©ë‹ˆë‹¤.

ì—¬ê¸°ì„œ ì»¤ë„ íŠ¸ë¦­(Kernel trick) í•¨ìˆ˜ë€ ì €ì°¨ì›ì˜ ë°ì´í„°ë¥¼ ê³ ì°¨ì›ì˜ ê³µê°„ì— ë§¤í•‘ì‹œì¼œ ì£¼ëŠ” í•¨ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ ë•Œ, ê³ ì°¨ì›ì—ì„œ ë°ì´í„°ëŠ” í•­ìƒ ë‘ ë²¡í„°ê°„ì˜ ë‚´ì ìœ¼ë¡œë§Œ ì¡´ì¬í•˜ë¯€ë¡œ ì´ëŸ¬í•œ ì»¤ë„ íŠ¸ë¦­ í•¨ìˆ˜ì˜ ì¢…ë¥˜ëŠ” ë‹¤ì–‘í•˜ê²Œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì»¤ë„ íŠ¸ë¦­ í•¨ìˆ˜ëŠ” ë˜í•œ ë‹¨ì§€ ë‘ ë²¡í„°ê°„ì˜ ë‚´ì ì„ ê³„ì‚°í• ìˆ˜ ìˆì–´ì•¼í•  ë¿ë§Œ ì•„ë‹ˆë¼ ì•„ë˜ì˜ Mercer's Theoremì„ ë§Œì¡±í•´ì•¼í•©ë‹ˆë‹¤. í•´ë‹¹ ì´ë¡ ì€ ì•„ë˜ ê·¸ë¦¼ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”.
![image](https://user-images.githubusercontent.com/68594529/199635742-b840bfeb-ddfe-4901-b31e-88d1d7ab603c.png)<br>
ì¶œì²˜ : https://sonsnotation.blogspot.com/2020/11/11-1-kernel.html
í•´ë‹¹ ì •ë¦¬ë¥¼ ìš”ì•½í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
-> Kernel í•¨ìˆ˜ K ê°€ ì‹¤ìˆ˜ scalar ë¥¼ ì¶œë ¥í•˜ëŠ” continuous functionì¼ ê²ƒ <br>
-> Kernel í•¨ìˆ˜ê°’ìœ¼ë¡œ ë§Œë“  í–‰ë ¬ì´ Symmetric(ëŒ€ì¹­í–‰ë ¬)ì´ë‹¤.<br>
-> Positive semi-definite(ëŒ€ê°ì›ì†Œ>0)ë¼ë©´ $K(xi, xj) = K(xj, xi) = <Î¦(xi), Î¦(xj)>$ë¥¼ ë§Œì¡±í•˜ëŠ” mapping Î¦ ê°€ ì¡´ì¬í•œë‹¤. ì¦‰, Reproducing kernel Hilbert spaceë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.

ìœ„ì™€ ê°™ì€ ì •ë¦¬ë¥¼ ë§Œì¡±í•˜ëŠ” ëŒ€í‘œì ì¸ kernerl íŠ¸ë¦­ í•¨ìˆ˜ì˜ ì¢…ë¥˜ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
- Polynomial : $K(x,y) = ( x \cdot y + c) ^d
- Linear : $K(x,y) = (x \cdot y^T)
- Gaussian(RBF) : $exp(-\frac {||x-y||^2} {2\sigma^2})$

#### Python code
```python
import numpy as np

class SVM:
  #kernel í•¨ìˆ˜
  def __init__(self, kernel='linear', C=10000.0, max_iter=100000, degree=3, gamma=1):
    self.kernel = {'poly'  : lambda x,y: np.dot(x, y.T)**degree,
                   'rbf'   : lambda x,y: np.exp(-gamma*np.sum((y - x[:,np.newaxis])**2, axis=-1)),
                   'linear': lambda x,y: np.dot(x, y.T)}[kernel]
    #ì˜¤ë¶„ë¥˜ ë¹„ìš© C
    self.C = C
    #ë°˜ë³µ ì‹œí–‰ íšŸìˆ˜
    self.max_iter = max_iter
  # np.clip(array, min, max)í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ squareë¡œ ë³€í™˜í•¨
  # ìµœì ì˜ ì„ ì„ ì°¾ê¸° ìœ„í•œ ë°˜ë³µ ìˆ˜í–‰ ê³¼ì •ì¤‘ square ì¬êµ¬ì¶• ê³¼ì •(min,max ë²—ì–´ë‚˜ëŠ” ê°’ ì¬êµ¬ì¶•ë¨)
  def restrict_to_square(self, t, v0, u):
    t = (np.clip(v0 + t*u, 0, self.C) - v0)[1]/u[1]
    return (np.clip(v0 + t*u, 0, self.C) - v0)[0]/u[0]
  # Optimization
  def fit(self, X, y):
    self.X = X.copy()
    self.y = y * 2 - 1
    self.lambdas = np.zeros_like(self.y, dtype=float)
    self.K = self.kernel(self.X, self.X) * self.y[:,np.newaxis] * self.y
    
    #ë°˜ë³µ ìˆ˜í–‰í•˜ë©° ìµœì ì˜ ë¶„ë¥˜ ê²½ê³„ë©´ì„ êµ¬í•¨
    for _ in range(self.max_iter):
      for idxM in range(len(self.lambdas)):
        idxL = np.random.randint(0, len(self.lambdas))
        Q = self.K[[[idxM, idxM], [idxL, idxL]], [[idxM, idxL], [idxM, idxL]]]
        v0 = self.lambdas[[idxM, idxL]]
        k0 = 1 - np.sum(self.lambdas * self.K[[idxM, idxL]], axis=1)
        u = np.array([-self.y[idxL], self.y[idxM]])
        t_max = np.dot(k0, u) / (np.dot(np.dot(Q, u), u) + 1E-15)
        self.lambdas[[idxM, idxL]] = v0 + u * self.restrict_to_square(t_max, v0, u)
    
    idx, = np.nonzero(self.lambdas > 1E-15)
    self.b = np.mean((1.0 - np.sum(self.K[idx] * self.lambdas, axis=1)) * self.y[idx])
  
  #ìµœì¢… ë¶„ë¥˜ë©´ 
  def decision_function(self, X):
    return np.sum(self.kernel(X, self.X) * self.y * self.lambdas, axis=1) + self.b
  
  #ì˜ˆì¸¡ ì‹œí–‰
  def predict(self, X):
    return (np.sign(self.decision_function(X)) + 1) // 2
```
ë‹¤ìŒìœ¼ë¡œëŠ” Sklearnì˜ wrapper ëª¨ë¸ì¸ SVCì™€ ì„±ëŠ¥ ë¹„êµë¥¼ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤.

ìœ„ì˜ íŒŒì´ì¬ ì½”ë“œë¥¼ í†µí•´ SVM ë¶„ë¥˜ê¸°ë¥¼ ë§Œë“¤ì–´ë³´ì•˜ìŠµë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ê³¼ì—° í•´ë‹¹ ì½”ë“œì™€ ì‹¤ì œ Sklearnì˜ SVCì™€ì˜ ë¹„êµë¥¼ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤.
- ìš°ì„  ë¶„ë¥˜ ê²½ê³„ë©´ì„ ìƒì„±í•˜ê³  ì´ë¥¼ ë¹„êµí•˜ê¸° ìœ„í•œ test_plot í•¨ìˆ˜ë¥¼ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤.
```python
def test_plot(X, y, svm_model, axes, title):
  plt.axes(axes)
  xlim = [np.min(X[:, 0]), np.max(X[:, 0])]
  ylim = [np.min(X[:, 1]), np.max(X[:, 1])]
  xx, yy = np.meshgrid(np.linspace(*xlim, num=700), np.linspace(*ylim, num=700))
  rgb=np.array([[210, 0, 0], [0, 0, 150]])/255.0
  start_time = time.time()
  svm_model.fit(X, y)
  z_model = svm_model.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
  end_time = time.time()
  print("WorkingTime %s time : %s sec" % (svm_model, end_time-start_time))
  plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
  plt.contour(xx, yy, z_model, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])
  plt.contourf(xx, yy, np.sign(z_model.reshape(xx.shape)), alpha=0.3, levels=2, cmap=ListedColormap(rgb), zorder=1)
  plt.title(title)
```
ìœ„ì˜ í•¨ìˆ˜ë¥¼ í† ëŒ€ë¡œ ê²°ì • ê²½ê³„ë©´ì´ ì–´ë–»ê²Œ í˜•ì„±ë˜ëŠ”ì§€ ì°¨ì´ë¥¼ ë³´ê³  ì¶”ê°€ì ìœ¼ë¡œ í•¨ìˆ˜ ì•Œê³ ë¦¬ì¦˜ì˜ ì‹œê°„ì  ì°¨ì´ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.
ë°ì´í„°ì…‹ ì˜ˆì‹œ : ì›í˜• ë°ì´í„°, ì„ í˜• ë°ì´í„°, ë¹„ì„ í˜• ë°ì´í„° 
```python
import time
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
from sklearn.datasets import make_blobs, make_circles
from matplotlib.colors import ListedColormap

X, y = make_circles(100, factor=.1, noise=.1)
fig, axs = plt.subplots(nrows=1,ncols=2,figsize=(12,4))
test_plot(X, y, SVM(kernel='rbf', C=10, max_iter=60, gamma=1), axs[0], 'OUR ALGORITHM')
test_plot(X, y, SVC(kernel='rbf', C=10, gamma=1), axs[1], 'sklearn.svm.SVC')

X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=1.4)
fig, axs = plt.subplots(nrows=1,ncols=2,figsize=(12,4))
test_plot(X, y, SVM(kernel='linear', C=10, max_iter=60), axs[0], 'Our Algorithm')
test_plot(X, y, SVC(kernel='linear', C=10), axs[1], 'sklearn.svm.SVC')

fig, axs = plt.subplots(nrows=1,ncols=2,figsize=(12,4))
test_plot(X, y, SVM(kernel='poly', C=5, max_iter=60, degree=3), axs[0], 'Our Algorithm')
test_plot(X, y, SVC(kernel='poly', C=5, degree=3), axs[1], 'sklearn.svm.SVC')
```
||Sklearn svm|our svm|
|:---:|:---:|:---:|
|ì›í˜• ë°ì´í„°|0.285 sec|1.761 sec|
|ì„ í˜• ë°ì´í„°|0.185 sec|0.289 sec|
|ë¹„ì„ í˜• ë°ì´í„°|0.177 sec|0.743 sec|

- ì›í˜• ë°ì´í„°ì…‹ result
![image](https://user-images.githubusercontent.com/68594529/199649349-e4d3b6b6-bdf7-412f-8804-433227c48267.png)
- ì„ í˜• ë°ì´í„°ì…‹
![image](https://user-images.githubusercontent.com/68594529/199649471-91c760d1-c0f4-46cf-8a7c-c157ec49c7d6.png)
- ë¹„ì„ í˜• ë°ì´í„°ì…‹
![image](https://user-images.githubusercontent.com/68594529/199649486-24affd05-20f8-4e18-84c7-a532170444e0.png)

í•´ë‹¹ ê²°ê³¼ë¥¼ ë³´ë©´ skleanì˜ svcì˜ ì„±ëŠ¥ì´ ìµœì†Œ 2ë°°ì—ì„œ 5ë°°ê¹Œì§€ ì°¨ì´ê°€ ë‚¨ì„ í™•ì¸í• ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. í•´ë‹¹ ì´ìœ ëŠ” Sklearn ì˜ defalut ì„¤ì • ë•Œë¬¸ì¸ê²ƒ ê°™ìŠµë‹ˆë‹¤.
- class sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)
- skleanrì˜ default parameter ì„¤ì •ì„ ë³´ë©´ shrinkingì´ë€ ì„¤ì •ì´ defalutë¡œ Trueê°€ ë˜ì–´ ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. sklearnì—ì„œ ì°¸ê³  í•œ í•´ë‹¹ ë…¼ë¬¸ì„ ë³´ë©´(https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf) , itertationì´ ì»¤ì§ˆìˆ˜ë¡ shrinkingì´ ì»¤ì§ˆìˆ˜ë¡ training timeì´ ì¤„ì–´ë“ ë‹¤ ë˜ì–´ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ ë¶€ë¶„ì´ ì €ì˜ ë…¼ë¬¸ì—ì„œëŠ” êµ¬í˜„ì´ ë˜ì–´ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì— ì„±ëŠ¥ ì°¨ì´ê°€ ë°œìƒí•˜ì˜€ë‹¤ íŒë‹¨í•˜ì˜€ìŠµë‹ˆë‹¤.

---

## Feedback

- í•´ë‹¹ íŠœí† ë¦¬ì–¼ì€ SVM ëª¨ë¸ ê·¸ì¤‘ì—ì„œë„ ì§ê´€ì ì¸ classificiationì— ì´ˆì ì„ ë§ì¶° ì§„í–‰í•¨ìœ¼ë¡œì¨ regression ë° anamoly detectionì—ì„œì˜ svmì˜ ì„±ëŠ¥ ê²€ì¦ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.
- ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œì˜ ì„±ëŠ¥ê²€ì¦ì´ ë˜ì—ˆë‹¤ë©´ ì‚¬ìš©ìë“¤ì´ ìƒí™©ì— ë§ê²Œ ì„ íƒí•˜ì—¬ í•˜ì´í¼ íŒŒë¼ë¯¸í„°(C, Gamma)ë¥¼ ì„ íƒí•  ìˆ˜ ìˆì—ˆì„ê²ƒ ê°™ìŠµë‹ˆë‹¤.

---
## References
[ê³ ë ¤ëŒ€í•™êµ ê°•í•„ì„± êµìˆ˜ë‹˜](https://github.com/pilsung-kang)<br>
[Sklearn - SVM](https://scikit-learn.org/stable/modules/svm.html#shrinking-svm)<br>
[Shrinking paper] (https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf)


